{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:50.188826Z","iopub.execute_input":"2025-07-11T03:52:50.189152Z","iopub.status.idle":"2025-07-11T03:52:50.193893Z","shell.execute_reply.started":"2025-07-11T03:52:50.189128Z","shell.execute_reply":"2025-07-11T03:52:50.193129Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:53.241356Z","iopub.execute_input":"2025-07-11T03:52:53.241694Z","iopub.status.idle":"2025-07-11T03:52:56.543081Z","shell.execute_reply.started":"2025-07-11T03:52:53.241672Z","shell.execute_reply":"2025-07-11T03:52:56.542198Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# 导包\nfrom transformers import AutoModelForTokenClassification , AutoTokenizer \nfrom transformers import DataCollatorForTokenClassification , TrainingArguments , Trainer\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate   # pip install evaluate\nimport seqeval  # pip install seqeval\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\nimport torch\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:12.604244Z","iopub.execute_input":"2025-07-11T04:33:12.604519Z","iopub.status.idle":"2025-07-11T04:33:12.609963Z","shell.execute_reply.started":"2025-07-11T04:33:12.604500Z","shell.execute_reply":"2025-07-11T04:33:12.609276Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n\n    # 直接从数据集中提取已有标签（避免与原数据不一致）\n    tags = [\n        'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n    ]\n    # 提取实体类型 PER/ORG/LOC 及 O\n    entity_types = ['O'] + sorted(list(set(tag.split('-')[-1] for tag in tags if tag != 'O')))\n\n    entity_index = {entity: i for i, entity in enumerate(entity_types)}\n    \n    return ds, tags, entity_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:53:03.106289Z","iopub.execute_input":"2025-07-11T03:53:03.106993Z","iopub.status.idle":"2025-07-11T03:53:03.111965Z","shell.execute_reply.started":"2025-07-11T03:53:03.106966Z","shell.execute_reply":"2025-07-11T03:53:03.110984Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# 加载数据\nds = load_dataset('doushabao4766/msra_ner_k_V3')\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n\nfor items in ds['train']:\n    print(items['tokens'])\n    print(items['ner_tags'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:25.087007Z","iopub.execute_input":"2025-07-11T04:33:25.087476Z","iopub.status.idle":"2025-07-11T04:33:26.981898Z","shell.execute_reply.started":"2025-07-11T04:33:25.087454Z","shell.execute_reply":"2025-07-11T04:33:26.981299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9548a09824490dad1aecda6a4c228b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4075ddfe1b492ab82488876615c00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61a0b59283a4f49a9957c61c9333f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2f4f0e37354a2193cc352739fb93a8"}},"metadata":{}},{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# 查看tag标签数量\ntags_id = set()\nfor tags in ds['train']:\n    tags_id.update(tags['ner_tags'])\n\ntags_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:38.002719Z","iopub.execute_input":"2025-07-11T04:33:38.003257Z","iopub.status.idle":"2025-07-11T04:33:42.938877Z","shell.execute_reply.started":"2025-07-11T04:33:38.003235Z","shell.execute_reply":"2025-07-11T04:33:42.938146Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{0, 1, 2, 3, 4, 5, 6}"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# 构建映射标签\nentites = list({'per' , 'loc' , 'org'})\ntags = ['O']\nfor entity in entites:\n    tags.append('B-' + entity.upper())  # upper()方法是转换为大写\n    tags.append('I-' + entity.upper())\ntags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:48.643263Z","iopub.execute_input":"2025-07-11T04:33:48.643954Z","iopub.status.idle":"2025-07-11T04:33:48.649125Z","shell.execute_reply.started":"2025-07-11T04:33:48.643928Z","shell.execute_reply":"2025-07-11T04:33:48.648465Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"\n# 创建构建方法 [tag + [0] * (512 - len(tag)) for tag in item['ner_tags']]\ndef data_input_proc(item):\n    input_data = tokenizer(item['tokens'],\n                          truncation = True ,  # 超过最大长度允许截断防止溢出\n                          max_length = 512 ,   #最大512\n                          add_special_tokens = False ,  # 禁止添加特殊标记  确保标签对其\n                          is_split_into_words = True) # 因为该数据集已经按照字符划分，所以用id_split_into_words = True 表明一个字符一个字符的传入\n    # 设置标签映射（超过512 截断）\n    labels = [lbl[:512] for lbl in item['ner_tags']]\n    input_data['labels'] = labels\n    return input_data\nds1 = ds.map(data_input_proc , batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:56.903526Z","iopub.execute_input":"2025-07-11T04:33:56.904194Z","iopub.status.idle":"2025-07-11T04:34:08.124203Z","shell.execute_reply.started":"2025-07-11T04:33:56.904171Z","shell.execute_reply":"2025-07-11T04:34:08.123463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca554c08e10c42f595ddcb34b775ab51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a754c958c94958b8f0444151445022"}},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"for item in ds1['train']:\n    print(item['tokens'])\n    print(item['ner_tags'])\n    print(item['knowledge'])\n    print(item['input_ids'])\n    print(item['token_type_ids'])\n    print(item['attention_mask'])\n    print(item['labels'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:10.019631Z","iopub.execute_input":"2025-07-11T04:34:10.019909Z","iopub.status.idle":"2025-07-11T04:34:10.025816Z","shell.execute_reply.started":"2025-07-11T04:34:10.019888Z","shell.execute_reply":"2025-07-11T04:34:10.025118Z"}},"outputs":[{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# 选择模型需要输入的列 将其转换为 torch张量类型\nds1.set_format('torch' , columns = ['input_ids' ,  # token 索引序列\n                                    'token_type_ids' ,  # 段落标记\n                                    'attention_mask' ,  # 注意力掩码\n                                    'labels']) # NER标签序列\nfor item in ds1['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:21.677378Z","iopub.execute_input":"2025-07-11T04:34:21.677652Z","iopub.status.idle":"2025-07-11T04:34:21.692598Z","shell.execute_reply.started":"2025-07-11T04:34:21.677632Z","shell.execute_reply":"2025-07-11T04:34:21.692000Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# 构建模型初始化可读标签参数，\nid2lbl = {i:tag for i,tag in enumerate(tags)}\nlbl2id = {tag:i for i,tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\" , # 预训练模型\n                                                       num_labels = len(tags) ,  # 输出的分类数量\n                                                       id2label = id2lbl , \n                                                       label2id = lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:30.750940Z","iopub.execute_input":"2025-07-11T04:34:30.751284Z","iopub.status.idle":"2025-07-11T04:34:33.718485Z","shell.execute_reply.started":"2025-07-11T04:34:30.751256Z","shell.execute_reply":"2025-07-11T04:34:33.717755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ceb7175ade44949818d0fb56a7eada"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# 自动填充对其\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)\n# 在DataLoader中使用\ntrain_dl = DataLoader(\n    ds1['train'], \n    batch_size=16,\n    shuffle = True,\n    collate_fn = data_collator\n)\n\n\nmodel.to('cuda')\n\n# 模型参数分组\n\n# 获取模型参数\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    # 获取预训练模型\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},  # 预训练模型的学习率较低 保持稳定性\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3} # 新的分类层学习率较高 更好的学习，'weight_decay':0.1 使用正则化L2\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\n\n# 步长 从初始设置值到0 衰减需要的步长\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            # 预热 从0到初始设置值的步长\n                                            num_warmup_steps=100, \n                                            # 衰减 从初始设置值到0 衰减需要的步长\n                                            num_training_steps=train_steps)\n\n\nfor item in train_dl:\n    print(item['input_ids'].shape, \n          item['token_type_ids'].shape, \n          item['attention_mask'].shape, \n          item['labels'].shape)\n    break\n\nDEVICE='cuda'\n\nfor epoch in range(5):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        # 张量移动到指定的设备商\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        # 数据传入模型\n        outputs = model(**items)\n        # 计算损失\n        loss = outputs.loss\n        # 反向传播计算梯度\n        loss.backward()\n        # 更新模型参数的梯度\n        optimizer.step()\n        # 更新学习率\n        scheduler.step()\n        # 模型参数的梯度清零\n        optimizer.zero_grad()\n    \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:50.779389Z","iopub.execute_input":"2025-07-11T04:34:50.780154Z","iopub.status.idle":"2025-07-11T05:57:56.561393Z","shell.execute_reply.started":"2025-07-11T04:34:50.780124Z","shell.execute_reply":"2025-07-11T05:57:56.560768Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90])\n","output_type":"stream"},{"name":"stderr","text":"Epoch:1 bert_lr:8.057286072323668e-06 classifier_lr:0.0008057286072323666 Loss:0.0498: 100%|██████████| 2813/2813 [16:35<00:00,  2.83it/s]\nEpoch:2 bert_lr:6.042964554242751e-06 classifier_lr:0.000604296455424275 Loss:0.0105: 100%|██████████| 2813/2813 [16:37<00:00,  2.82it/s]  \nEpoch:3 bert_lr:4.028643036161834e-06 classifier_lr:0.0004028643036161833 Loss:0.0121: 100%|██████████| 2813/2813 [16:38<00:00,  2.82it/s]  \nEpoch:4 bert_lr:2.014321518080917e-06 classifier_lr:0.00020143215180809166 Loss:0.0001: 100%|██████████| 2813/2813 [16:33<00:00,  2.83it/s] \nEpoch:5 bert_lr:0.0 classifier_lr:0.0 Loss:0.0001: 100%|██████████| 2813/2813 [16:41<00:00,  2.81it/s]                                      \n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def compute_metric(result):\n    # 传入的result是一个元祖 (predicts,labels)\n\n    # 加载序列标注评估指标库\n    seqeval = evaluate.load('seqeval')\n    # 解构模型输出的结果\n    predicts,labels = result\n    # 沿着axis = 2 的维度 取最大值索引 然后将predicts转换为预测标签ID\n    predicts = np.argmax(predicts , axis = 2)\n    # 准备评估数据 将数字ID转换为文本标签 并且过滤填充数值-100\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions = predicts , references = labels)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:52.359981Z","iopub.execute_input":"2025-07-11T06:36:52.360592Z","iopub.status.idle":"2025-07-11T06:36:52.365753Z","shell.execute_reply.started":"2025-07-11T06:36:52.360568Z","shell.execute_reply":"2025-07-11T06:36:52.364957Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:55.953626Z","iopub.execute_input":"2025-07-11T06:36:55.953883Z","iopub.status.idle":"2025-07-11T06:36:55.957721Z","shell.execute_reply.started":"2025-07-11T06:36:55.953862Z","shell.execute_reply":"2025-07-11T06:36:55.956898Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir = 'ner_train' , # 设置模型输出目录\n    num_train_epochs = 3 , # 训练轮数\n    #save_safetensor = False # 模型禁止保存safe格式 可以用troch.load加载\n    per_device_train_batch_size = 32 , # 训练批次\n    per_device_eval_batch_size = 32 ,  # 评估批次\n    report_to = 'tensorboard' , # 设置训练输出记录为tensorboard\n    eval_strategy = 'epoch'  # 每轮评估一次\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:58.449836Z","iopub.execute_input":"2025-07-11T06:36:58.450070Z","iopub.status.idle":"2025-07-11T06:36:58.484696Z","shell.execute_reply.started":"2025-07-11T06:36:58.450053Z","shell.execute_reply":"2025-07-11T06:36:58.484042Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"trainer = Trainer(\n    model = model ,  # 指定模型\n    args = args , # 指定设置参数\n    train_dataset = ds1['train'] ,  # 输入训练数据\n    eval_dataset = ds1['test'] ,  # 输入评估数据\n    compute_metrics = compute_metric , # 指定评估函数\n    data_collator = data_collator  # 指定数据收集器\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:00.324017Z","iopub.execute_input":"2025-07-11T06:37:00.324243Z","iopub.status.idle":"2025-07-11T06:37:00.601153Z","shell.execute_reply.started":"2025-07-11T06:37:00.324227Z","shell.execute_reply":"2025-07-11T06:37:00.600605Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:03.217858Z","iopub.execute_input":"2025-07-11T06:37:03.218114Z","iopub.status.idle":"2025-07-11T07:15:47.910873Z","shell.execute_reply.started":"2025-07-11T06:37:03.218094Z","shell.execute_reply":"2025-07-11T07:15:47.910342Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2112/2112 38:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc</th>\n      <th>Org</th>\n      <th>Per</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014400</td>\n      <td>0.033448</td>\n      <td>{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}</td>\n      <td>{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}</td>\n      <td>{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}</td>\n      <td>0.913897</td>\n      <td>0.935154</td>\n      <td>0.924403</td>\n      <td>0.991583</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010200</td>\n      <td>0.031717</td>\n      <td>{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}</td>\n      <td>{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}</td>\n      <td>{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}</td>\n      <td>0.936107</td>\n      <td>0.939736</td>\n      <td>0.937918</td>\n      <td>0.993087</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.003000</td>\n      <td>0.038207</td>\n      <td>{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}</td>\n      <td>{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}</td>\n      <td>{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}</td>\n      <td>0.938890</td>\n      <td>0.944846</td>\n      <td>0.941858</td>\n      <td>0.993504</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0cedcd9a404e53a6d18bfd84adb77a"}},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2112, training_loss=0.008169087835333565, metrics={'train_runtime': 2324.0708, 'train_samples_per_second': 58.089, 'train_steps_per_second': 0.909, 'total_flos': 1.180990200098808e+16, 'train_loss': 0.008169087835333565, 'epoch': 3.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"from transformers import pipeline\n\npipeline = pipeline('token-classification', 'ner_train/checkpoint-2112')\n\ntext = pipeline('双方确定了今后发展中美关系的指导方针')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:51.301395Z","iopub.execute_input":"2025-07-11T07:18:51.302135Z","iopub.status.idle":"2025-07-11T07:18:51.547399Z","shell.execute_reply.started":"2025-07-11T07:18:51.302110Z","shell.execute_reply":"2025-07-11T07:18:51.546822Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":81}]}
